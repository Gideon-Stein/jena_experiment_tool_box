{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f44934a",
   "metadata": {},
   "source": [
    "# All steps in order from raw to processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec3e61f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from os import listdir\n",
    "from os.path import isdir, isfile, join\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy_indexed as npi\n",
    "from GP.tools import *\n",
    "\n",
    "import numpy as np; np.random.seed(0)\n",
    "import seaborn as sns; sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23ec2db",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f560318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_paths(): \n",
    "    path2 =\"../Canbus_data/raw\"\n",
    "    years = sorted(listdir(path2))\n",
    "    split = []\n",
    "    for x in years: \n",
    "        split.append(sorted([f for f in listdir(path2 + \"/\" + x) if isdir(join(path2 + \"/\" + x, f))]))\n",
    "    tables = []\n",
    "    for x in range(len(years)):\n",
    "        current_year = []\n",
    "        for y in split[x]: \n",
    "            current_year.append(sorted([path2 + \"/\" + years[x] + \"/\" + y + \"/\" + f for f in listdir(path2 + \"/\" + years[x] + \"/\" + y) if isfile(join(path2 + \"/\" + years[x] + \"/\" + y, f))]))\n",
    "        tables.append([item for sublist in current_year for item in sublist])\n",
    "    tables_list = [item for sublist in tables for item in sublist]\n",
    "    return tables,tables_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0656a897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ts(ts_name):\n",
    "    print(\"building timeseries: \" + str(ts_name))\n",
    "    print(\"loading rounded timestamps...\")\n",
    "    time = pickle.load(open(\"raw_stats/timeLineRounded.p\", \"rb\"))\n",
    "    print(\"loading columns information...\")\n",
    "    wrongColumns = pickle.load(open(\"raw_stats/mappingWrongColumns.p\", \"rb\")) \n",
    "    allColumns = list(pd.read_csv(\"Canbus_Data/Complete-HeaderFile_extendedBasedOn20173020605_V4.csv\", encoding= 'unicode_escape').columns)\n",
    "    print(\"loading main data file...\")\n",
    "    data  = pd.read_csv(\"data_transform/restructured_raw/table_\" + str(allColumns.index(ts_name)) + \".csv\")\n",
    "    addData = []\n",
    "    for x in wrongColumns[ts_name]:  \n",
    "        print(\"loading additional column: \" + str(x) + \"...\")\n",
    "        addData.append(pd.read_csv(\"data_transform/restructured_raw/fColumns/\" + str(x) + \".csv\" , encoding= 'unicode_escape'))\n",
    "        addData[-1][\"t[s]\"] = pd.to_datetime(round_time(addData[-1][\"t[s]\"]))\n",
    "    missingVals = sum(np.isnan(data[data.columns[0]]))\n",
    "    print(\"Data has \" + str(missingVals) + \" rows that are  empty\")\n",
    "    for x in range(len(addData)):\n",
    "        assert list(set(addData[x][\"t[s]\"]) - set(time)) == [], \"Unknown time was discovered in additional column\" \n",
    "        nonEmpty = len(addData[x]) - sum(np.isnan(addData[x][addData[x].columns[1]]))\n",
    "        print(\"Additional column \" + str(wrongColumns[ts_name][x]) + \"has \" + str(nonEmpty) + \" non empty values\")\n",
    "    mergeOut = data.columns[0]\n",
    "    data[\"t[s]\"] = pd.to_datetime(time)\n",
    "    for x in range(len(addData)):   # should be tested for multiple column inserts!\n",
    "        data = data.merge(addData[x],left_on=\"t[s]\", right_on = \"t[s]\",how= \"outer\")\n",
    "        data[mergeOut] = np.where(np.isnan(data[wrongColumns[ts_name][x]]), data[mergeOut],data[wrongColumns[ts_name][x]])\n",
    "    missingVals_2 = sum(np.isnan(data[mergeOut]))\n",
    "    print(\"Data has \" + str(missingVals_2) + \" rows that are empty after merch (\" + str(missingVals - missingVals_2) \n",
    "          + \" values were added through wrong column merges).\") \n",
    "    print(\"Spreading time...\")\n",
    "    data = spread_time(data[[\"t[s]\",mergeOut]])\n",
    "    print(\"len after merging and spreading: \" + str(len(data)))\n",
    "    print(\"dropping duplicate timestamps...\")\n",
    "    data.drop_duplicates(subset=\"t[s]\", inplace=True, ignore_index=True)\n",
    "    print(\"len after dropping: \" + str(len(data)))\n",
    "    print(\"Loading done.\")\n",
    "    return data\n",
    "\n",
    "def load_277_278(ts_name):   #Bug catch. these variables does only exist with wrong names in the original data\n",
    "    time = pickle.load(open(\"raw_stats/timeLineRounded.p\", \"rb\"))\n",
    "    wrongColumns = pickle.load(open(\"raw_stats/mappingWrongColumns.p\", \"rb\")) \n",
    "    allColumns = list(pd.read_csv(\"Canbus_Data/Complete-HeaderFile_extendedBasedOn20173020605_V4.csv\", encoding= 'unicode_escape').columns)\n",
    "    data = pd.DataFrame(time, columns = [\"t[s]\"])\n",
    "    addData = []\n",
    "    for x in wrongColumns[ts_name]:  \n",
    "        print(\"loading additional column: \" + str(x) + \"...\")\n",
    "        addData.append(pd.read_csv(\"data_transform/restructured_raw/fColumns/\" + str(x) + \".csv\" , encoding= 'unicode_escape'))\n",
    "        addData[-1][\"t[s]\"] =  pd.to_datetime(addData[-1][\"t[s]\"], format=\"%d.%m.%Y %H:%M:%S.%f\").dt.floor('Min')\n",
    "    data = data.merge(addData[0],left_on=\"t[s]\", right_on = \"t[s]\",how= \"outer\")\n",
    "    data = data.merge(addData[1],left_on=\"t[s]\", right_on = \"t[s]\",how= \"outer\")\n",
    "    data[ts_name] = np.where(np.isnan(data[data.columns[2]]), data[data.columns[1]], data[data.columns[2]])\n",
    "    data = spread_time(data[[\"t[s]\",ts_name]])\n",
    "    data.drop_duplicates(subset=\"t[s]\", inplace=True)\n",
    "    print(\"len after merging and spreading: \" + str(len(data)))\n",
    "    print(\"dropping duplicate timestamps...\")\n",
    "    data.drop_duplicates(subset=\"t[s]\", inplace=True, ignore_index=True)\n",
    "    print(\"len after dropping: \" + str(len(data)))\n",
    "    print(\"Loading done.\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4016f89",
   "metadata": {},
   "source": [
    "# Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3a009a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All paths\n",
    "tables,tables_list = get_all_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aff27815",
   "metadata": {},
   "outputs": [],
   "source": [
    "allColumns = list(pd.read_csv(\"../Canbus_data/Complete-HeaderFile_extendedBasedOn20173020605_V4.csv\", encoding= 'unicode_escape').columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a4d17c",
   "metadata": {},
   "source": [
    "## Transform the data into complete timeseries and correct time differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e877a1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "which = np.concatenate(([allColumns[0]], allColumns[1:50]))    #I've split the loading into multiple blocks due to ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d147016",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n"
     ]
    }
   ],
   "source": [
    "# load a bundle of columns\n",
    "missing = []\n",
    "container = []\n",
    "counter = 0\n",
    "for x in tables_list: \n",
    "    counter += 1\n",
    "    if counter % 1000 == 0: \n",
    "        print(counter)\n",
    "    try: \n",
    "        toParse = []\n",
    "        data  = pd.read_csv(x, encoding= 'unicode_escape')\n",
    "        for x in which: \n",
    "            if x in data.columns: \n",
    "                toParse.append(x)\n",
    "        container.append(data[toParse])\n",
    "    except:\n",
    "        container.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb5cb03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the corrected tables were the time difference is too high between name and first index\n",
    "timeProblems = pickle.load(open(\"raw_stats/corrected_list.p\", \"rb\"))\n",
    "for x in timeProblems: \n",
    "    where = tables_list.index(x)\n",
    "    new = pd.read_csv(\"data_transform/time_changes/\" + x.split(\"/\")[-1], encoding= 'unicode_escape')\n",
    "    toParse = []\n",
    "    for x in which: \n",
    "        if x in new.columns: \n",
    "            toParse.append(x)\n",
    "    container[where] = new[toParse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b13118cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(container)): \n",
    "    if len(container[x]) == 0: \n",
    "        container[x] = pd.DataFrame([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8386960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a single list out of the container: \n",
    "table = pd.concat(container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46725408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8522674, 50)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b648bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(0,50): \n",
    "    a = table[which[x+ 1:x + 2]].reset_index(drop=True)\n",
    "    a.to_csv(\"data_transform/table_\" + str(x+1) + \".csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f03e30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one time timeline\n",
    "timeLine = []\n",
    "for x in container: \n",
    "    if len(x) == 0 :\n",
    "        pass\n",
    "    else:\n",
    "        timeLine.append(x[\"t[s]\"])\n",
    "timeLine = [item for sublist in timeLine for item in sublist]\n",
    "pickle.dump(timeLine, open(\"data_transform/timeLine.p\", \"wb\"))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2721cbcb",
   "metadata": {},
   "source": [
    "## Build all broken column names as tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b1e0846",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional = pickle.load(open(\"raw_stats/additionalColumns.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aeef27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = pickle.load(open(\"raw_stats/additionalColumnNames.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f68c592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = {}\n",
    "for key in additional:\n",
    "    box = []\n",
    "    for value in additional[key]:\n",
    "        if \"Unnamed\" in value: \n",
    "            pass\n",
    "        else: \n",
    "            box.append(value)\n",
    "    if box: \n",
    "        filtered[key] = box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8664ba9d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n"
     ]
    }
   ],
   "source": [
    "#load all errors\n",
    "timeProblems = pickle.load(open(\"raw_stats/corrected_list.p\", \"rb\"))\n",
    "container = {}\n",
    "counter = 0\n",
    "for x in names: \n",
    "    container[x] = []\n",
    "for x in list(filtered.keys()): \n",
    "    counter += 1\n",
    "    if counter % 1000 == 0: \n",
    "        print(counter)\n",
    "    if x not in timeProblems:\n",
    "        data  = pd.read_csv(x, encoding= 'unicode_escape')\n",
    "    else: \n",
    "        data  = pd.read_csv(\"data_transform/time_changes/\" + x.split(\"/\")[-1], encoding= 'unicode_escape')    \n",
    "    for y in names: \n",
    "        if y in data.columns: \n",
    "            container[y].append(data[[\"t[s]\", y]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "74d4ddec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in container.keys(): \n",
    "    pd.concat(container[x]).to_csv(\"data_transform/fColumns/\" + str(x) + \".csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a10decb",
   "metadata": {},
   "source": [
    "# To create the prepared dataset: \n",
    "    - run \"load_ts\" on any column name to get the final timeseries\n",
    "    - run two simple filters\n",
    "    - save the data and the filtered value indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef4f6d6",
   "metadata": {},
   "source": [
    "### load_ts performs\n",
    "- Load main file with data including timer corrections based on file names\n",
    "- Adds wrong column name files to the dataset\n",
    "- Checks for some stats before merging\n",
    "- Rounds time to seconds (00)\n",
    "- Spreading the dataset to the full timescale\n",
    "- filter duplicate timestamps\n",
    "- Outputs data + time in final format\n",
    "- some intermediate tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eec379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant = []\n",
    "for x in allColumns: \n",
    "    if 'spare_temp_OR_volt' in x: \n",
    "        pass\n",
    "    elif 'underpress_suction_plates' in x: \n",
    "        pass\n",
    "    elif 'underpress_suc_plates' in x: \n",
    "        pass\n",
    "    elif \"air_rel_humidiy_30cm\" in x: \n",
    "        pass\n",
    "    elif x == \"t[s]\":\n",
    "        pass\n",
    "    else:\n",
    "        relevant.append(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb847b39-b256-4903-bdb6-987c29e486c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#redo the % variables with a different filter\n",
    "redo = []\n",
    "for x in allColumns: \n",
    "    if \"air_rel_humidiy_30cm\" in x:\n",
    "        redo.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17c87a39-2cdb-41e6-a1ee-53bdb3537fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#redo the sur variables with a different filter\n",
    "redo = []\n",
    "for x in allColumns: \n",
    "    if \"surface_temp_south_west\" in x:\n",
    "        redo.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84518498-8c5a-4964-90d8-ebf90ee467c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant.index(\"soil_temp_15cm_depth_3A06N[°C]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa576a6-ed04-4e76-9d74-57d2de12510d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "for x in relevant[:180] + relevant[182:]: \n",
    "    corrected = {}\n",
    "    ts1 = load_ts(x)\n",
    "    outliers = threshold_filter(ts1,threshold=45)\n",
    "    corrected[\"threshold\"] = outliers.values\n",
    "    set_outliers_nan(ts1,outliers)\n",
    "    pickle.dump(ts1[x], open(\"data_transform/prepared_filtered/\" + x + \".p\", \"wb\"))\n",
    "    pickle.dump(corrected, open(\"data_transform/prepared_filtered/filter_\" + x + \".p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80bf0f6f-b048-4025-a250-554466cf7611",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building timeseries: air_rel_humidiy_30cm_hei_2A01M[%]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "Data has 268295 rows that are  empty\n",
      "Data has 268295 rows that are empty after merch (0 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426612\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: air_rel_humidiy_30cm_hei_2A02M[%]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "Data has 125444 rows that are  empty\n",
      "Data has 125444 rows that are empty after merch (0 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426612\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: air_rel_humidiy_30cm_hei_2A03M[%]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "Data has 277779 rows that are  empty\n",
      "Data has 277779 rows that are empty after merch (0 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426612\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: air_rel_humidiy_30cm_hei_2A04M[%]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "Data has 322242 rows that are  empty\n",
      "Data has 322242 rows that are empty after merch (0 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426612\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: air_rel_humidiy_30cm_hei_2A05M[%]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "Data has 126656 rows that are  empty\n",
      "Data has 126656 rows that are empty after merch (0 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426612\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: air_rel_humidiy_30cm_hei_2A06M[%]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "Data has 380310 rows that are  empty\n",
      "Data has 380310 rows that are empty after merch (0 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426612\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: air_rel_humidiy_30cm_hei_2A07M[%]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "Data has 1298728 rows that are  empty\n",
      "Data has 1298728 rows that are empty after merch (0 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426612\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: air_rel_humidiy_30cm_hei_2A08M[%]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "Data has 320632 rows that are  empty\n",
      "Data has 320632 rows that are empty after merch (0 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426612\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: air_rel_humidiy_30cm_hei_2A09M[%]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "Data has 421392 rows that are  empty\n",
      "Data has 421392 rows that are empty after merch (0 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426612\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: air_rel_humidiy_30cm_hei_2A10M[%]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "Data has 258231 rows that are  empty\n",
      "Data has 258231 rows that are empty after merch (0 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426612\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: air_rel_humidiy_30cm_hei_2A11M[%]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "Data has 625559 rows that are  empty\n",
      "Data has 625559 rows that are empty after merch (0 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426612\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: air_rel_humidiy_30cm_hei_2A12M[%]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "Data has 825768 rows that are  empty\n",
      "Data has 825768 rows that are empty after merch (0 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426612\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: air_rel_humidiy_30cm_hei_2A13M[%]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "Data has 907545 rows that are  empty\n",
      "Data has 907545 rows that are empty after merch (0 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426612\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: air_rel_humidiy_30cm_hei_2A14M[%]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "Data has 688588 rows that are  empty\n",
      "Data has 688588 rows that are empty after merch (0 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426612\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: air_rel_humidiy_30cm_hei_2A15M[%]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "Data has 683715 rows that are  empty\n",
      "Data has 683715 rows that are empty after merch (0 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426612\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: air_rel_humidiy_30cm_hei_2A16M[%]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "Data has 881447 rows that are  empty\n",
      "Data has 881447 rows that are empty after merch (0 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426612\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: air_rel_humidiy_30cm_hei_2A17M[%]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "Data has 870997 rows that are  empty\n",
      "Data has 870997 rows that are empty after merch (0 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426612\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: air_rel_humidiy_30cm_hei_2A18M[%]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "Data has 1488378 rows that are  empty\n",
      "Data has 1488378 rows that are empty after merch (0 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426612\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: air_rel_humidiy_30cm_hei_2A19M[%]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "Data has 711866 rows that are  empty\n",
      "Data has 711866 rows that are empty after merch (0 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426612\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: air_rel_humidiy_30cm_hei_2A20M[%]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "Data has 700340 rows that are  empty\n",
      "Data has 700340 rows that are empty after merch (0 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426612\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: air_rel_humidiy_30cm_hei_2A21M[%]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "Data has 1022109 rows that are  empty\n",
      "Data has 1022109 rows that are empty after merch (0 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426612\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: air_rel_humidiy_30cm_hei_2A22M[%]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "Data has 1137289 rows that are  empty\n",
      "Data has 1137289 rows that are empty after merch (0 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426612\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n"
     ]
    }
   ],
   "source": [
    " #%%capture\n",
    "for x in redo: \n",
    "    corrected = {}\n",
    "    ts1 = load_ts(x)\n",
    "    outliers = threshold_filter_2(ts1,threshold=(10,100))\n",
    "    corrected[\"threshold\"] = outliers.values\n",
    "    set_outliers_nan(ts1,outliers)\n",
    "    pickle.dump(ts1[x], open(\"data_transform/prepared_filtered/\" + x + \".p\", \"wb\"))\n",
    "    pickle.dump(corrected, open(\"data_transform/prepared_filtered/filter_\" + x + \".p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa4c5fb5-7121-4ec7-89a3-af1d79d788a4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building timeseries: surface_temp_south_west_2A01M[°C]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "loading additional column: surface_temp_south_west_2A01M[_]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stein/envs/idiv/lib/python3.6/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 657396 rows that are  empty\n",
      "Additional column surface_temp_south_west_2A01M[_]has 67840 non empty values\n",
      "Data has 589556 rows that are empty after merch (67840 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426976\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: surface_temp_south_west_2A02M[°C]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "loading additional column: surface_temp_south_west_2A02M[_]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stein/envs/idiv/lib/python3.6/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 514545 rows that are  empty\n",
      "Additional column surface_temp_south_west_2A02M[_]has 67840 non empty values\n",
      "Data has 446705 rows that are empty after merch (67840 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426976\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: surface_temp_south_west_2A03M[°C]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "loading additional column: surface_temp_south_west_2A03M[_]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stein/envs/idiv/lib/python3.6/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 666880 rows that are  empty\n",
      "Additional column surface_temp_south_west_2A03M[_]has 67840 non empty values\n",
      "Data has 599040 rows that are empty after merch (67840 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426976\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: surface_temp_south_west_2A04M[°C]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "loading additional column: surface_temp_south_west_2A04M[_]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stein/envs/idiv/lib/python3.6/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 711343 rows that are  empty\n",
      "Additional column surface_temp_south_west_2A04M[_]has 67840 non empty values\n",
      "Data has 643503 rows that are empty after merch (67840 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426976\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: surface_temp_south_west_2A05M[°C]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "loading additional column: surface_temp_south_west_2A05M[_]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stein/envs/idiv/lib/python3.6/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 515757 rows that are  empty\n",
      "Additional column surface_temp_south_west_2A05M[_]has 67840 non empty values\n",
      "Data has 447917 rows that are empty after merch (67840 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426976\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: surface_temp_south_west_2A06M[°C]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "loading additional column: surface_temp_south_west_2A06M[_]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stein/envs/idiv/lib/python3.6/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 769411 rows that are  empty\n",
      "Additional column surface_temp_south_west_2A06M[_]has 67840 non empty values\n",
      "Data has 701571 rows that are empty after merch (67840 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426976\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: surface_temp_south_west_2A07M[°C]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "loading additional column: surface_temp_south_west_2A07M[_]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stein/envs/idiv/lib/python3.6/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 1687829 rows that are  empty\n",
      "Additional column surface_temp_south_west_2A07M[_]has 67840 non empty values\n",
      "Data has 1619989 rows that are empty after merch (67840 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426976\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: surface_temp_south_west_2A08M[°C]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "loading additional column: surface_temp_south_west_2A08M[_]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stein/envs/idiv/lib/python3.6/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 709733 rows that are  empty\n",
      "Additional column surface_temp_south_west_2A08M[_]has 67840 non empty values\n",
      "Data has 641893 rows that are empty after merch (67840 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426976\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: surface_temp_south_west_2A09M[°C]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "loading additional column: surface_temp_south_west_2A09M[_]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stein/envs/idiv/lib/python3.6/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 810493 rows that are  empty\n",
      "Additional column surface_temp_south_west_2A09M[_]has 67840 non empty values\n",
      "Data has 742653 rows that are empty after merch (67840 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426976\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: surface_temp_south_west_2A10M[°C]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "loading additional column: surface_temp_south_west_2A10M[_]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stein/envs/idiv/lib/python3.6/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 647332 rows that are  empty\n",
      "Additional column surface_temp_south_west_2A10M[_]has 67840 non empty values\n",
      "Data has 579492 rows that are empty after merch (67840 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426976\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: surface_temp_south_west_2A11M[°C]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "loading additional column: surface_temp_south_west_2A11M[_]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stein/envs/idiv/lib/python3.6/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 1014660 rows that are  empty\n",
      "Additional column surface_temp_south_west_2A11M[_]has 67840 non empty values\n",
      "Data has 946820 rows that are empty after merch (67840 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426976\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: surface_temp_south_west_2A12M[°C]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "loading additional column: surface_temp_south_west_2A12M[_]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stein/envs/idiv/lib/python3.6/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 1190419 rows that are  empty\n",
      "Additional column surface_temp_south_west_2A12M[_]has 67840 non empty values\n",
      "Data has 1122579 rows that are empty after merch (67840 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426976\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: surface_temp_south_west_2A13M[°C]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "loading additional column: surface_temp_south_west_2A13M[_]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stein/envs/idiv/lib/python3.6/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 1291370 rows that are  empty\n",
      "Additional column surface_temp_south_west_2A13M[_]has 67840 non empty values\n",
      "Data has 1223530 rows that are empty after merch (67840 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426976\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: surface_temp_south_west_2A14M[°C]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "loading additional column: surface_temp_south_west_2A14M[_]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stein/envs/idiv/lib/python3.6/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 1072413 rows that are  empty\n",
      "Additional column surface_temp_south_west_2A14M[_]has 67840 non empty values\n",
      "Data has 1004573 rows that are empty after merch (67840 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426976\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: surface_temp_south_west_2A15M[°C]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "loading additional column: surface_temp_south_west_2A15M[_]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stein/envs/idiv/lib/python3.6/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 1067540 rows that are  empty\n",
      "Additional column surface_temp_south_west_2A15M[_]has 67840 non empty values\n",
      "Data has 999700 rows that are empty after merch (67840 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426976\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: surface_temp_south_west_2A16M[°C]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "loading additional column: surface_temp_south_west_2A16M[_]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stein/envs/idiv/lib/python3.6/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 1255984 rows that are  empty\n",
      "Additional column surface_temp_south_west_2A16M[_]has 58552 non empty values\n",
      "Data has 1197432 rows that are empty after merch (58552 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426924\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: surface_temp_south_west_2A17M[°C]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "loading additional column: surface_temp_south_west_2A17M[_]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stein/envs/idiv/lib/python3.6/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 1254822 rows that are  empty\n",
      "Additional column surface_temp_south_west_2A17M[_]has 67840 non empty values\n",
      "Data has 1186982 rows that are empty after merch (67840 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426976\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: surface_temp_south_west_2A18M[°C]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "loading additional column: surface_temp_south_west_2A18M[_]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stein/envs/idiv/lib/python3.6/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 1862915 rows that are  empty\n",
      "Additional column surface_temp_south_west_2A18M[_]has 58552 non empty values\n",
      "Data has 1804363 rows that are empty after merch (58552 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426924\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: surface_temp_south_west_2A19M[°C]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "loading additional column: surface_temp_south_west_2A19M[_]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stein/envs/idiv/lib/python3.6/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 1095691 rows that are  empty\n",
      "Additional column surface_temp_south_west_2A19M[_]has 67840 non empty values\n",
      "Data has 1027851 rows that are empty after merch (67840 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426976\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: surface_temp_south_west_2A20M[°C]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "loading additional column: surface_temp_south_west_2A20M[_]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stein/envs/idiv/lib/python3.6/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 1084165 rows that are  empty\n",
      "Additional column surface_temp_south_west_2A20M[_]has 67840 non empty values\n",
      "Data has 1016325 rows that are empty after merch (67840 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426976\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: surface_temp_south_west_2A21M[°C]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "loading additional column: surface_temp_south_west_2A21M[_]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stein/envs/idiv/lib/python3.6/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 1393062 rows that are  empty\n",
      "Additional column surface_temp_south_west_2A21M[_]has 67840 non empty values\n",
      "Data has 1325222 rows that are empty after merch (67840 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426976\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n",
      "building timeseries: surface_temp_south_west_2A22M[°C]\n",
      "loading rounded timestamps...\n",
      "loading columns information...\n",
      "loading main data file...\n",
      "loading additional column: surface_temp_south_west_2A22M[_]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stein/envs/idiv/lib/python3.6/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 1510713 rows that are  empty\n",
      "Additional column surface_temp_south_west_2A22M[_]has 67840 non empty values\n",
      "Data has 1442873 rows that are empty after merch (67840 values were added through wrong column merges).\n",
      "Spreading time...\n",
      "len after merging and spreading: 9426976\n",
      "dropping duplicate timestamps...\n",
      "len after dropping: 9393633\n",
      "Loading done.\n"
     ]
    }
   ],
   "source": [
    " #%%capture 2\n",
    "for x in redo: \n",
    "    corrected = {}\n",
    "    ts1 = load_ts(x)\n",
    "    outliers = threshold_filter_2(ts1,threshold=(-45,55))\n",
    "    corrected[\"threshold\"] = outliers.values\n",
    "    set_outliers_nan(ts1,outliers)\n",
    "    pickle.dump(ts1[x], open(\"data_transform/prepared_filtered/\" + x + \".p\", \"wb\"))\n",
    "    pickle.dump(corrected, open(\"data_transform/prepared_filtered/filter_\" + x + \".p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77a61414-0c8b-4a11-85a5-038f2f733923",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#redo 277 and 278 with the new fix\n",
    "corrected = {}\n",
    "ts1 = load_277_278(allColumns[277])\n",
    "outliers = threshold_filter(ts1)\n",
    "corrected[\"threshold\"] = outliers.values\n",
    "set_outliers_nan(ts1,outliers)\n",
    "pickle.dump(ts1[allColumns[277]], open(\"data_transform/prepared_filtered/\" + allColumns[277] + \".p\", \"wb\"))\n",
    "pickle.dump(corrected, open(\"data_transform/prepared_filtered/filter_\" + allColumns[277] + \".p\", \"wb\"))\n",
    "corrected = {}\n",
    "ts1 = load_277_278(allColumns[278])\n",
    "outliers = threshold_filter(ts1,threshold=45)\n",
    "corrected[\"threshold\"] = outliers.values\n",
    "set_outliers_nan(ts1,outliers)\n",
    "pickle.dump(ts1[allColumns[278]], open(\"data_transform/prepared_filtered/\" + allColumns[278] + \".p\", \"wb\"))\n",
    "pickle.dump(corrected, open(\"data_transform/prepared_filtered/filter_\" + allColumns[278] + \".p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba58d45f-4956-443e-8e77-8322966ef062",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ts1[\"t[s]\"], open(\"data_transform/prepared_filtered/\" + \"time\" + \".p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2475dce2-e4d3-4c72-95e4-ebe02fea1ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct the filter table: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d96547f5-6d5d-4bcc-a9db-097b3737db81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cf491aa",
   "metadata": {},
   "source": [
    "# Todo: \n",
    "\n",
    "- Check if transform counts merge correctly\n",
    "- fix the warning\n",
    "- make a better threshold filter\n",
    "\n",
    "# Things to keep in mind:\n",
    "- There are 2239 timestamps that are duplicated but have different values? Not a lot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
